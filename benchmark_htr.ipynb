{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark HTR Models\n",
    "\n",
    "This notebook runs various vision-language models on historical French manuscript images and compares their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from utils import query_openrouter, system_prompt, generate_results_md_table\n",
    "\n",
    "# Create necessary directories if they don't exist\n",
    "results_dir = Path(\"résultats\")\n",
    "reference_dir = Path(\"transcriptions_de_référence\")\n",
    "images_dir = Path(\"images\")\n",
    "\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "reference_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models configuration\n",
    "with open(\"models_to_test.json\", \"r\") as f:\n",
    "    models = json.load(f)\n",
    "\n",
    "# Get list of images\n",
    "image_files = list(images_dir.glob(\"*.jpg\")) + list(images_dir.glob(\"*.png\"))\n",
    "print(f\"Found {len(image_files)} images to process\")\n",
    "print(f\"Will test {len(models)} models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to determine model metadata\n",
    "def get_model_metadata(model_id, model_type):\n",
    "    \"\"\"Extract editor and model type from model ID and type\"\"\"\n",
    "    parts = model_id.split(\"/\")\n",
    "    editor = parts[0]\n",
    "    \n",
    "    # Map model type to French\n",
    "    model_type_fr = \"libre\" if model_type == \"open\" else \"propriétaire\"\n",
    "    \n",
    "    return {\n",
    "        \"editeur\": editor,\n",
    "        \"modele_type\": model_type_fr\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each image with each model in parallel\n",
    "import concurrent.futures\n",
    "from functools import partial\n",
    "\n",
    "max_w = 1\n",
    "\n",
    "results = []\n",
    "\n",
    "def process_image(img_path, model_info):\n",
    "    model_meta = get_model_metadata(model_info[0], model_info[1])\n",
    "    model, _ = model_info\n",
    "    \n",
    "    # Create valid filename by replacing invalid characters\n",
    "    safe_model_name = model.replace('/', '_').replace('\\\\', '_').replace(':', '_')\n",
    "    result_file = results_dir / f\"{img_path.stem}_{safe_model_name}.json\"\n",
    "    \n",
    "    # Check if result file already exists and is valid JSON\n",
    "    if result_file.exists():\n",
    "        print(f\"Skipping existing result: {result_file}\")\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        # Query the model\n",
    "        response_data, cost = query_openrouter(str(img_path), model)\n",
    "        \n",
    "        # Extract the transcription from the response\n",
    "        transcription = response_data['choices'][0]['message']['content']\n",
    "        \n",
    "        # Get usage data with defaults if missing\n",
    "        usage_data = response_data.get('usage', {})\n",
    "        if not usage_data or not isinstance(usage_data, dict):\n",
    "            usage_data = {\n",
    "                'prompt_tokens': 0,\n",
    "                'completion_tokens': 0,\n",
    "                'total_tokens': 0\n",
    "            }\n",
    "\n",
    "        # Prepare result data\n",
    "        result_data = {\n",
    "            \"model\": model,\n",
    "            \"editeur\": model_meta[\"editeur\"],\n",
    "            \"modele_type\": model_meta[\"modele_type\"],\n",
    "            \"image\": str(img_path),\n",
    "            \"result\": transcription,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"model_info\": response_data.get('model_info', {}),\n",
    "            \"usage\": usage_data,\n",
    "            \"latency\": response_data.get('created') - response_data.get('started', 0) if 'created' in response_data else None\n",
    "        }\n",
    "        \n",
    "        # Save result to file\n",
    "        with open(result_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(result_data, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "        return result_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {img_path} with {model}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Process models in parallel\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=max_w) as executor:\n",
    "    for model_info in tqdm(models, desc=\"Processing models\"):\n",
    "        # Create partial function with fixed model_info\n",
    "        process_fn = partial(process_image, model_info=model_info)\n",
    "        \n",
    "        # Submit all images for this model to process in parallel\n",
    "        future_to_img = {executor.submit(process_fn, img_path): img_path \n",
    "                        for img_path in image_files}\n",
    "        \n",
    "        # Process results as they complete\n",
    "        for future in tqdm(concurrent.futures.as_completed(future_to_img), \n",
    "                         total=len(future_to_img),\n",
    "                         desc=f\"Processing images with {model_info[0]}\", \n",
    "                         leave=False):\n",
    "            result = future.result()\n",
    "            if result is not None:\n",
    "                results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the results table\n",
    "generate_results_md_table()\n",
    "\n",
    "# Also create a DataFrame for additional analysis\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Add columns for latency and token usage analysis\n",
    "if not df_results.empty:\n",
    "    df_summary = df_results.groupby('model').agg({\n",
    "        'latency': ['mean', 'min', 'max'],\n",
    "        'usage': lambda x: pd.Series([d.get('total_tokens', 0) for d in x]).mean()\n",
    "    }).round(2)\n",
    "    \n",
    "    df_summary.columns = ['Avg Latency (s)', 'Min Latency (s)', 'Max Latency (s)', 'Avg Tokens']\n",
    "    display(df_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Performance Metrics\n",
    "\n",
    "The summary above includes:\n",
    "- Latency metrics (average, min, max) in seconds\n",
    "- Average token usage per request\n",
    "\n",
    "These metrics complement the WER scores and cost analysis in the main results table, providing a more complete picture of each model's performance characteristics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
